{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a subset of information from the Twitter JSON Data\n",
    "\n",
    "In this example, we read a collection of timestamped Twitter data in JSON format collected using the Twitter Streaming API to collect tweets during the period of November 20 to December 5, 2018, searching on the hashtag 'climatechange'. The data in JSON format comprises ~3.13 Gb of Twitter data in uncompressed text format. Below we list some basic statistics of the #climatechange dataset.\n",
    "\n",
    "* Start: Tue Nov 20 00:07:52 +0000 2018\n",
    "* End: Wed Dec 05 18:38:47 +0000 2018\n",
    "* 457294 Total Tweets\n",
    "* 138191 Original Tweets\n",
    "* 319103 Retweets\n",
    "* 22580 Unique User Screen Names\n",
    "* 44127 Unique Retweets\n",
    "\n",
    "<b>NOTE</b>: Since Twitter imposes a limit of 50,000 full tweets for the number of tweets we are allowed to share in a single day, we do not actually provide the original JSON data for this example. However, the code below can be generalized to any Twitter data collected with the Streaming API and saved as JSON. We provide a subset of the JSON data (the two .json files provided in the data/twitter/ subdirectory containing 44,910 tweets) for you to work with in this lesson. Since we also provide the full dataset of 457,294 tweets in reduced format (i.e. the file in the data subdirectory called 'climatechange_tweets_all.csv', see below), we want to be sure when doing this lesson that you don't overwrite that file, so for this lesson we will name our output file 'climatechange_tweets_sample.csv'. \n",
    "\n",
    "As you have already seen, a tweet may consist of more than 40 fields. For our analysis we are only interested in a subset of fields (15). This should greatly reduce the resulting file size. You are of course encouraged to adapt this code to suit your specific needs if you want more (or less) than these 15 fields when analyzing your own data.\n",
    "\n",
    "The 15 fields we will be extracting are:\n",
    "\n",
    "<ul>\n",
    "    <li>id</li>\n",
    "    <li>created_at</li>\n",
    "    <li>lang</li>\n",
    "    <li>screen_name (user)</li>\n",
    "    <li>created_at (user)</li>\n",
    "    <li>id (user)</li>\n",
    "    <li>followers_count (user)</li>\n",
    "    <li>friends_count (user)</li>\n",
    "    <li>utc_offset (user)</li>\n",
    "    <li>time_zone (user)</li>\n",
    "    <li>text</li>\n",
    "</ul>\n",
    "\n",
    "and if the tweet is a retweet...\n",
    "<ul>\n",
    "    <li>retweeted_status</li>\n",
    "    <li>id</li>\n",
    "    <li>screen_name (user)</li>\n",
    "    <li>created_at (user)</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import glob, os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open JSON, Parse Data, Save as CSV\n",
    "\n",
    "In the sample JSON files provided,  we read the JSON data line-by-line using the <code>json.loads()</code> command. This is because each tweet is an individual line in the JSON text file. The comments in the code below describe the purpose of each section of code. Most but not all tweets are retweets (319,103). Some tweets are original tweets (138,191). Original tweets do not contain the same data as retweets. In particular, original tweets do not include the 'retweeted_status' object. As a result, if we want to include both original tweets and retweets in our dataframe, we need a way to handle the extra data found only in retweets. We do this with a check on the 'retweeted_status' key in the tweet data and deal with the four parameters (retweeted_status, retweet_id, retweet_user_screen_name, retweet_user_id) accordingly.\n",
    "\n",
    "We also include an optional counter which prints a statement as each 10,000th tweet is read, making it easy to track progress with large files without doing too much printing.\n",
    "\n",
    "The total time for this process was roughly 90 seconds. Saving this reduced data set in a CSV file will enable us to quickly load the data in a Pandas dataframe, which is useful for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:2019-12-09 16:15:59.041345\n",
      "Working on file:../data/twitter/climatechange_2018_11_26_17_19_15_679824.json\n",
      "10000\n",
      "20000\n",
      "Working on file:../data/twitter/climatechange_2018_11_27_11_30_06_972631.json\n",
      "30000\n",
      "40000\n",
      "End:2019-12-09 16:16:03.849446\n",
      "Total number of tweets: 44910\n"
     ]
    }
   ],
   "source": [
    "# Print start time at start and end time at end\n",
    "print(\"Start:\" + str(datetime.datetime.now()))\n",
    "\n",
    "# Open CSV output files for reading and writing\n",
    "input_dir = \"../data/twitter/\"\n",
    "output_dir = \"../data/twitter/\"\n",
    "hashtag = \"climatechange\"\n",
    "\n",
    "# Open main twitter data CSV file and write header row\n",
    "output_file = output_dir + hashtag + \"_tweets_sample.csv\"\n",
    "f_out = open(output_file, 'w', encoding='utf-8')\n",
    "rowwriter = csv.writer(f_out, delimiter=',', lineterminator='\\n')\n",
    "outputrow = ['tweet_id','tweet_created_at','language','user_screen_name','user_created_at','user_id','followers_count','friends_count','time_zone','utc_offset','retweeted_status','retweet_id','retweet_user_screen_name','retweet_user_id','text']\n",
    "rowwriter.writerow(outputrow)\n",
    "\n",
    "# Define variables\n",
    "inc = 0\n",
    "\n",
    "files = glob.glob(input_dir + '*.json')\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        print(\"Working on file:\" + file)\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            if 'user' in tweet:\n",
    "                \n",
    "                # Set standard variables equal to tweet data\n",
    "                tweet_id = tweet['id']\n",
    "                tweet_created_at = tweet['created_at']\n",
    "                text = tweet['text'].replace('\\r', ' ').replace('\\n', ' ')\n",
    "                language = tweet['lang']\n",
    "                user_screen_name = tweet['user']['screen_name']\n",
    "                user_created_at = tweet['user']['created_at']\n",
    "                user_id = tweet['user']['id']\n",
    "                followers_count = tweet['user']['followers_count']\n",
    "                friends_count = tweet['user']['friends_count']\n",
    "                utc_offset = tweet['user']['utc_offset']\n",
    "                time_zone = tweet['user']['time_zone']\n",
    "\n",
    "                # Check if a retweet else original tweet\n",
    "                if 'retweeted_status' in tweet:\n",
    "                    retweeted_status = 1\n",
    "                    retweet_id = tweet['retweeted_status']['id']\n",
    "                    retweet_user_screen_name = tweet['retweeted_status']['user']['screen_name']\n",
    "                    retweet_user_id = tweet['retweeted_status']['user']['id']\n",
    "                else:\n",
    "                    retweeted_status = 0\n",
    "                    retweet_id = \"None\"\n",
    "                    retweet_user_screen_name = \"None\"\n",
    "                    retweet_user_id = \"None\"\n",
    "\n",
    "                # Write to main output file\n",
    "                outputrow = [str(tweet_id), tweet_created_at, language, user_screen_name, user_created_at, str(user_id), str(followers_count), str(friends_count), time_zone, utc_offset, str(retweeted_status), str(retweet_id), retweet_user_screen_name, str(retweet_user_id), text] \n",
    "                rowwriter.writerow(outputrow)\n",
    "\n",
    "                inc += 1\n",
    "                # Optional counter increments variables to track progress, useful for very large files.\n",
    "                if inc%10000 == 0:\n",
    "                    print(inc)\n",
    "\n",
    "# Close the output file\n",
    "f_out.close()\n",
    "print(\"End:\" + str(datetime.datetime.now()))\n",
    "print(\"Total number of tweets: {}\".format(inc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweak the CSV file to accomodate timestamps\n",
    "\n",
    "The CSV file we just wrote is a much smaller distillation of the data of interest to us, but we'll want to tweak it a little bit so that we can work with it in pandas more easily.  \n",
    "\n",
    "Two of the fields in the CSV file represent timestamps: <code>tweet_created_at</code> and <code>user_created_at</code>.  The <code>pd.read_csv</code> function can be passed an additional argument indicating that particular columns should be parsed as dates (datetimes), instead of plain strings.  Unfortunately, the Twitter data that we extracted from the JSON file is in a format that is not the standard datetime format, e.g., <code>Thu Nov 29 19:22:55 +0000 2018</code>.  Pandas has a function named <code>to_datetime</code> that can not only convert strings to datetime objects, but can infer datetimes from a number of different formats.  That inference can be rather slow for a large file, however.\n",
    "\n",
    "Fortunately, the <code>pd.to_datetime</code> function can be provided with a specific format string, so that it does not need to infer a format.  By providing an explicit format string to the conversion function for our tweet timestamps, the conversion is quite fast (a few seconds). Format strings are based upon the Python strftime directives (link). If we do not provide a format hint of this sort, the conversion takes more than a minute for each function call.  In that code that follows, we:\n",
    "\n",
    "* read the CSV file into pandas\n",
    "* convert two of the fields that are timestamps to datetime objects with a specific format string\n",
    "* write out a new CSV file with the reformatted data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/twitter/\"\n",
    "tweet_df = pd.read_csv(output_dir + 'climatechange_tweets_sample.csv')\n",
    "tweet_df.tweet_created_at = pd.to_datetime(tweet_df.tweet_created_at, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "tweet_df.user_created_at = pd.to_datetime(tweet_df.user_created_at, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "tweet_df.to_csv(output_dir + 'climatechange_tweets_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working further with the reformatted CSV file\n",
    "\n",
    "Having tweaked the datetime formats, if we want to read our new CSV file into a pandas dataframe, we can augment the <code>pd.read_csv</code> call to specify which columns to parse as dates, using the <code>parse_dates</code> option.  If we had not previously altered the datetime formats, this call to <code>pd.read_csv</code> would be very slow due to the new to do datetime format inference.  Now that we have fixed the formats, the date parsing proceeds quickly.  The <code>pd.read_csv</code> function, however, does not allow one to specify a format string, which is why we needed to do the conversion with the <code>pd.to_datetime</code> function as above.  Here's what the new read_csv function call looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv(output_dir + 'climatechange_tweets_fixed.csv', \\\n",
    "                       parse_dates=['tweet_created_at', 'user_created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can view our resulting dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>language</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweet_user_screen_name</th>\n",
       "      <th>retweet_user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1067211197412388864</td>\n",
       "      <td>2018-11-27 00:19:11</td>\n",
       "      <td>en</td>\n",
       "      <td>MarieCo92176893</td>\n",
       "      <td>2014-05-06 01:55:58</td>\n",
       "      <td>2479088160</td>\n",
       "      <td>1874</td>\n",
       "      <td>260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067192127124164612</td>\n",
       "      <td>DocsEnvAus</td>\n",
       "      <td>1542665564</td>\n",
       "      <td>RT @DocsEnvAus: @susanprescott88 paediatrician...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1067211198842716162</td>\n",
       "      <td>2018-11-27 00:19:12</td>\n",
       "      <td>en</td>\n",
       "      <td>FocusOnQuality</td>\n",
       "      <td>2010-09-14 22:06:33</td>\n",
       "      <td>190809029</td>\n",
       "      <td>1745</td>\n",
       "      <td>2569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067163637847085058</td>\n",
       "      <td>RepLoisFrankel</td>\n",
       "      <td>1077121945</td>\n",
       "      <td>RT @RepLoisFrankel: Here’s the thing, @realDon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1067211205083709440</td>\n",
       "      <td>2018-11-27 00:19:13</td>\n",
       "      <td>en</td>\n",
       "      <td>JudyOsburn8</td>\n",
       "      <td>2018-07-21 03:28:19</td>\n",
       "      <td>1020510759384563712</td>\n",
       "      <td>158</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067149009867878400</td>\n",
       "      <td>jessphoenix2018</td>\n",
       "      <td>842550390818201600</td>\n",
       "      <td>RT @jessphoenix2018: If you're not willing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1067211213409566722</td>\n",
       "      <td>2018-11-27 00:19:15</td>\n",
       "      <td>en</td>\n",
       "      <td>Rajivsheoran5</td>\n",
       "      <td>2018-11-23 13:57:04</td>\n",
       "      <td>1065967469607747587</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067051467083669504</td>\n",
       "      <td>UNinIndia</td>\n",
       "      <td>1856588299</td>\n",
       "      <td>RT @UNinIndia: Climate change affects us all! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1067211218258018304</td>\n",
       "      <td>2018-11-27 00:19:16</td>\n",
       "      <td>en</td>\n",
       "      <td>PaDenys</td>\n",
       "      <td>2015-02-08 20:57:34</td>\n",
       "      <td>3013785847</td>\n",
       "      <td>686</td>\n",
       "      <td>764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>@WasilewskiTomek #ClimateChange in action. 4 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44905</td>\n",
       "      <td>1067782537362227200</td>\n",
       "      <td>2018-11-28 14:09:30</td>\n",
       "      <td>en</td>\n",
       "      <td>NeverForget56</td>\n",
       "      <td>2015-12-15 22:16:53</td>\n",
       "      <td>4496345055</td>\n",
       "      <td>2685</td>\n",
       "      <td>3831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#DemocratsAreDangerous #ClimateChange is a hoax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44906</td>\n",
       "      <td>1067782538742124546</td>\n",
       "      <td>2018-11-28 14:09:30</td>\n",
       "      <td>en</td>\n",
       "      <td>jbriiicpa</td>\n",
       "      <td>2009-05-17 19:11:06</td>\n",
       "      <td>40716404</td>\n",
       "      <td>309</td>\n",
       "      <td>559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067763679486197761</td>\n",
       "      <td>Janis_Kelly</td>\n",
       "      <td>18100204</td>\n",
       "      <td>RT @Janis_Kelly: The #USGCRP warns that #Clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44907</td>\n",
       "      <td>1067782545465581569</td>\n",
       "      <td>2018-11-28 14:09:31</td>\n",
       "      <td>en</td>\n",
       "      <td>RedPillDetox</td>\n",
       "      <td>2017-12-10 19:08:41</td>\n",
       "      <td>939934915629670411</td>\n",
       "      <td>4454</td>\n",
       "      <td>4484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067781725206515717</td>\n",
       "      <td>ArtMusicLife</td>\n",
       "      <td>30296735</td>\n",
       "      <td>RT @ArtMusicLife: I. Just. Can’t. #Resist #Cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44908</td>\n",
       "      <td>1067782548443602945</td>\n",
       "      <td>2018-11-28 14:09:32</td>\n",
       "      <td>en</td>\n",
       "      <td>2ysur2ysub</td>\n",
       "      <td>2009-04-15 05:01:59</td>\n",
       "      <td>31345344</td>\n",
       "      <td>12255</td>\n",
       "      <td>11341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>921762776133046273</td>\n",
       "      <td>RichardAngwin</td>\n",
       "      <td>459269265</td>\n",
       "      <td>RT @RichardAngwin: Trump's energy policy  #Sat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44909</td>\n",
       "      <td>1067782560435056643</td>\n",
       "      <td>2018-11-28 14:09:35</td>\n",
       "      <td>en</td>\n",
       "      <td>ypp_peat</td>\n",
       "      <td>2012-02-17 15:27:32</td>\n",
       "      <td>495071155</td>\n",
       "      <td>686</td>\n",
       "      <td>451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1067699132129783808</td>\n",
       "      <td>PenninePeatLIFE</td>\n",
       "      <td>918494609952641024</td>\n",
       "      <td>RT @PenninePeatLIFE: Our next #webinar in part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44910 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id    tweet_created_at language user_screen_name  \\\n",
       "0      1067211197412388864 2018-11-27 00:19:11       en  MarieCo92176893   \n",
       "1      1067211198842716162 2018-11-27 00:19:12       en   FocusOnQuality   \n",
       "2      1067211205083709440 2018-11-27 00:19:13       en      JudyOsburn8   \n",
       "3      1067211213409566722 2018-11-27 00:19:15       en    Rajivsheoran5   \n",
       "4      1067211218258018304 2018-11-27 00:19:16       en          PaDenys   \n",
       "...                    ...                 ...      ...              ...   \n",
       "44905  1067782537362227200 2018-11-28 14:09:30       en    NeverForget56   \n",
       "44906  1067782538742124546 2018-11-28 14:09:30       en        jbriiicpa   \n",
       "44907  1067782545465581569 2018-11-28 14:09:31       en     RedPillDetox   \n",
       "44908  1067782548443602945 2018-11-28 14:09:32       en       2ysur2ysub   \n",
       "44909  1067782560435056643 2018-11-28 14:09:35       en         ypp_peat   \n",
       "\n",
       "          user_created_at              user_id  followers_count  \\\n",
       "0     2014-05-06 01:55:58           2479088160             1874   \n",
       "1     2010-09-14 22:06:33            190809029             1745   \n",
       "2     2018-07-21 03:28:19  1020510759384563712              158   \n",
       "3     2018-11-23 13:57:04  1065967469607747587                1   \n",
       "4     2015-02-08 20:57:34           3013785847              686   \n",
       "...                   ...                  ...              ...   \n",
       "44905 2015-12-15 22:16:53           4496345055             2685   \n",
       "44906 2009-05-17 19:11:06             40716404              309   \n",
       "44907 2017-12-10 19:08:41   939934915629670411             4454   \n",
       "44908 2009-04-15 05:01:59             31345344            12255   \n",
       "44909 2012-02-17 15:27:32            495071155              686   \n",
       "\n",
       "       friends_count  time_zone  utc_offset  retweeted_status  \\\n",
       "0                260        NaN         NaN                 1   \n",
       "1               2569        NaN         NaN                 1   \n",
       "2                198        NaN         NaN                 1   \n",
       "3                 16        NaN         NaN                 1   \n",
       "4                764        NaN         NaN                 0   \n",
       "...              ...        ...         ...               ...   \n",
       "44905           3831        NaN         NaN                 0   \n",
       "44906            559        NaN         NaN                 1   \n",
       "44907           4484        NaN         NaN                 1   \n",
       "44908          11341        NaN         NaN                 1   \n",
       "44909            451        NaN         NaN                 1   \n",
       "\n",
       "                retweet_id retweet_user_screen_name     retweet_user_id  \\\n",
       "0      1067192127124164612               DocsEnvAus          1542665564   \n",
       "1      1067163637847085058           RepLoisFrankel          1077121945   \n",
       "2      1067149009867878400          jessphoenix2018  842550390818201600   \n",
       "3      1067051467083669504                UNinIndia          1856588299   \n",
       "4                     None                     None                None   \n",
       "...                    ...                      ...                 ...   \n",
       "44905                 None                     None                None   \n",
       "44906  1067763679486197761              Janis_Kelly            18100204   \n",
       "44907  1067781725206515717             ArtMusicLife            30296735   \n",
       "44908   921762776133046273            RichardAngwin           459269265   \n",
       "44909  1067699132129783808          PenninePeatLIFE  918494609952641024   \n",
       "\n",
       "                                                    text  \n",
       "0      RT @DocsEnvAus: @susanprescott88 paediatrician...  \n",
       "1      RT @RepLoisFrankel: Here’s the thing, @realDon...  \n",
       "2      RT @jessphoenix2018: If you're not willing to ...  \n",
       "3      RT @UNinIndia: Climate change affects us all! ...  \n",
       "4      @WasilewskiTomek #ClimateChange in action. 4 c...  \n",
       "...                                                  ...  \n",
       "44905   #DemocratsAreDangerous #ClimateChange is a hoax.  \n",
       "44906  RT @Janis_Kelly: The #USGCRP warns that #Clima...  \n",
       "44907  RT @ArtMusicLife: I. Just. Can’t. #Resist #Cli...  \n",
       "44908  RT @RichardAngwin: Trump's energy policy  #Sat...  \n",
       "44909  RT @PenninePeatLIFE: Our next #webinar in part...  \n",
       "\n",
       "[44910 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the subsequent material (found in the notebook 'climatechange_tweet_timelines.ipynb') will use the CSV file containing the entire dataset of 457,294 tweets for further analysis and visualization, which are stored in a file named 'climatechange_tweets_all.csv'.  The date reformatting described above has already been performed for that data file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
